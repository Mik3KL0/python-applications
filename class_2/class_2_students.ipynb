{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2\n",
    "\n",
    "## Websraping\n",
    "---\n",
    "\n",
    "Web scraping is the process of extracting data from websites automatically, typically using software or scripts, rather than by manual copying and pasting. This involves accessing the HTML code of a webpage and identifying the specific data to be extracted, such as text, images, or other media. Web scraping can be used for a variety of purposes, including data analysis, market research, and content aggregation. However, it is important to note that web scraping can raise ethical and legal concerns, particularly if it involves accessing or using proprietary or copyrighted information without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevTools and site source code\n",
    "\n",
    "- Keyboard: `Ctrl + Shift + I`, except\n",
    "    - Internet Explorer and Edge: `F12`\n",
    "    - macOS: `⌘ + ⌥ + I`\n",
    "- Menu bar:\n",
    "    - Firefox: Menu   ➤ Web Developer ➤ Toggle Tools, or Tools ➤ Web Developer ➤ Toggle Tools\n",
    "    - Chrome: More tools ➤ Developer tools\n",
    "    - Safari: Develop ➤ Show Web Inspector. If you can't see the Develop menu, go to Safari ➤ Preferences ➤ Advanced, and check the Show Develop menu in menu bar checkbox.\n",
    "    -  Opera: Developer ➤ Developer tools\n",
    "- Context menu: Press-and-hold/right-click an item on a webpage (Ctrl-click on the Mac), and choose Inspect Element from the context menu that appears. (An added bonus: this method straight-away highlights the code of the element you right-clicked.)\n",
    "\n",
    "These tools are necessary for front-end development. They have use cases in Web Scraping, since we are going to use UI interactions.\n",
    "\n",
    "[Link to visit](https://pl.wikipedia.org/wiki/Wikipedia:Strona_g%C5%82%C3%B3wna)\n",
    "\n",
    "[DevTools in different browsers](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_are_browser_developer_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "All websites use HTML to work. \n",
    "Browser then interprets HTML file and creates DOM. Pure HTML websites look like skeleton [Example](http://info.cern.ch/hypertext/WWW/TheProject.html)\n",
    "By adding CSS website have styles, logic is created in a Javascript layer\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Attributes for HTML tags [Global attributes](https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes).\n",
    "\n",
    "Accessibility:\n",
    "- [Highly Accessible Website - All Public Websites](https://www.gov.pl/)\n",
    "\n",
    "### HTML5 and semantic tags\n",
    "\n",
    "Semantic elements of websites [Semantics](https://developer.mozilla.org/en-US/docs/Glossary/Semantics)\n",
    "\n",
    "### Differences of websites for every country\n",
    "\n",
    "Compare these 3 websites: they are owned by a single company but there are differences on them - website is maintained differently for each region\n",
    "\n",
    "- https://www.ebay.de/\n",
    "- https://www.ebay.pl/\n",
    "- https://www.ebay.com/\n",
    "\n",
    "### Protection against web scraping\n",
    "\n",
    "- https://datadome.co/bot-management-protection/scraper-crawler-bots-how-to-protect-your-website-against-intensive-scraping\n",
    "- https://medium.com/swlh/how-to-protect-your-web-application-from-web-scraping-cc01ec7ddadd\n",
    "- Example https://www.x-kom.pl/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E-commerce website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just made a request. [More about HTTP methods](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods)\n",
    "To communicate with the source and retrieve resources we need to perform GET request.\n",
    "`res` is a Response object. Hover your pointer over `res` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create BeautifulSoup object from received HTML source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a popular Python library used for web scraping. It allows you to parse HTML and XML documents, and navigate through their contents in a structured way.\n",
    "\n",
    "\n",
    "#### Get page title\n",
    "\n",
    "It's the same title as we can find in our Tab name in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.get_text()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between `find` and `find_all` methods\n",
    "`find` returns first found item that matches given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 = soup.find('h2')\n",
    "h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_all` returns list of all tags that match given argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1s = soup.find_all('h1')\n",
    "print(type(h1s))\n",
    "\n",
    "h1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the type of the iterated value\n",
    "h1: element.Tag\n",
    "\n",
    "for h1 in h1s:\n",
    "    print(h1.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect all links in Sidebar navigation\n",
    "\n",
    "`find` and `find_all` methods receive an optional second argument.\n",
    "\n",
    "In this argument we pass a dictionary with defined HTML tag attributes we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the wrapper of side-menu\n",
    "sidemenu = soup.find('ul', {'id': 'side-menu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to execute BeautifulSoup methods on a piece of taken HTML.\n",
    "\n",
    "We are working on a `side-menu` list. Let's take a peek how does it look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidemenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every tag can have attributes like `class`, `id`, `aria-*` or `data-*` which is additional information for the browser on how it should be presented.\n",
    "\n",
    "In our case it is really useful, because we can search for HTML tags with certain attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sidemenu.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all links and save them to a variable\n",
    "anchors = sidemenu.find_all('a')\n",
    "links = []\n",
    "link: element.Tag\n",
    "for link in anchors:\n",
    "    links.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for navigation links during web scraping is useful when building web crawlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">**Task 1**</span>\n",
    "\n",
    "Our client asked us to collect the products sold on [Laptop Subpage](https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops) website.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- full name of the laptop,\n",
    "- description of an item\n",
    "- price in dollars, \n",
    "- number of reviews,\n",
    "- overall rating\n",
    "\n",
    "1. Find out what is the average price of laptops on this website.\n",
    "2. Sort models by rating to price ratio and find the best deal.\n",
    "3. Visualize number of laptops for each rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://webscraper.io/test-sites/e-commerce/allinone/computers/laptops'\n",
    "html = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(html._content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium - Human interactions on the browser\n",
    "\n",
    "Used for dynamic websites, websites behind login forms, websites with unpredictable urls.\n",
    "\n",
    "In this chapter we want to automate interaction on the browser. \n",
    "Sometimes certain pages cannot be easily accessed - i.e they stay behind login forms, \n",
    "exist under unpredictable url or have to be accessed via click of the button.\n",
    "\n",
    "In this case we need to use Selenium library.\n",
    "\n",
    "Let's download **webdriver** of your current browser.\n",
    "\n",
    "[Installation guide for selenium](https://selenium-python.readthedocs.io/installation.html#)\n",
    "\n",
    "[Drivers](https://selenium-python.readthedocs.io/installation.html#drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://pythonscraping.com/linkedin/cookies/login.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element(by='name', value='username')\n",
    "username_input.send_keys('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element('name', 'password')\n",
    "username_input.send_keys('password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = driver.find_element(By.XPATH, \"//input[@type='submit']\")\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the link that has content `Check out your profile!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = driver.find_element(By.XPATH, \"//*[text() = 'Check out your profile!']\")\n",
    "link.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://www.booking.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Cookies banner\n",
    "cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "cookie_banner.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "search_input.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input.send_keys('Warszawa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "datepicker.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "start_date_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "filter_4_star.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data\n",
    "soup = BeautifulSoup(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "prop: element.Tag\n",
    "hotels = list()\n",
    "for prop in properties:\n",
    "  hotels.append({\n",
    "    'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "    'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "    'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "    'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "    'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "  })\n",
    "  \n",
    "pd.DataFrame(hotels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a single piece of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(source, city: str):\n",
    "    soup = BeautifulSoup(source)\n",
    "    properties = soup.find_all('div', {'data-testid': 'property-card'})\n",
    "    hotels = list()\n",
    "    for prop in properties:\n",
    "        try:\n",
    "            hotels.append({\n",
    "                'city': city,\n",
    "                'title':  prop.find('div', {'data-testid': 'title'}).text,\n",
    "                'address': prop.find('span', {'data-testid': 'address'}).text,\n",
    "                'distance': prop.find('span', {'data-testid': 'distance'}).text,\n",
    "                'price': prop.find('span', {'data-testid': 'price-and-discounted-price'}).text,\n",
    "                'review': prop.find('div', {'data-testid': 'review-score'}).find('div').text\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "    return hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_booking(city: str):\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.booking.com/')\n",
    "    try:\n",
    "        time.sleep(3)  \n",
    "        # Close Cookies banner - it blocks some visibility\n",
    "        cookie_banner = driver.find_element(By.ID,'onetrust-accept-btn-handler')\n",
    "        cookie_banner.click()\n",
    "        \n",
    "        search_input = driver.find_element(By.XPATH,\"//input[@name='ss']\")\n",
    "        search_input.click()\n",
    "        time.sleep(1)  \n",
    "        search_input.send_keys(city)\n",
    "        time.sleep(1)          \n",
    "        # Select date\n",
    "        datepicker = driver.find_element(By.XPATH,'//div[@data-testid=\"searchbox-dates-container\"]')\n",
    "        datepicker.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='20 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        start_date_button = driver.find_element(By.XPATH,\"//span[@aria-label='23 March 2023']\")\n",
    "        start_date_button.click()\n",
    "        time.sleep(1)  \n",
    "        \n",
    "        # Press search\n",
    "        search_button = driver.find_element(By.XPATH,\"//button[@type='submit']\")\n",
    "        search_button.click()\n",
    "        \n",
    "        # Filter only 4 star hotels\n",
    "        filter_4_star = driver.find_element(By.XPATH,\"//div[text()='4 stars']\")\n",
    "\n",
    "        ActionChains(driver).move_to_element(filter_4_star).perform()\n",
    "\n",
    "        filter_4_star.click()\n",
    "\n",
    "        # Wait until the results are visible\n",
    "        WebDriverWait(driver, 10).until(EC.invisibility_of_element_located((By.XPATH, \"//div[@data-testid='overlay-wrapper']\")))\n",
    "        # Extract data and save it in the dataframe\n",
    "        \n",
    "        data = collect_data(driver.page_source, city)\n",
    "        driver.quit()\n",
    "        return data\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_list = ['Warszawa']\n",
    "hotels = pd.DataFrame()\n",
    "\n",
    "for city in cities_list:\n",
    "    hotels = hotels.append(scrape_booking(city), ignore_index=True)\n",
    "    \n",
    "hotels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">**TASK & HOMEWORK**</span>\n",
    "\n",
    "Automate the process of getting details for restaurants from [TripAdvisor](https://www.tripadvisor.com/) website in Warsaw. \n",
    "\n",
    "Get data for first 3 result pages.\n",
    "\n",
    "Gather following data and save it to a pandas DataFrame:\n",
    "- name of the restaurant,\n",
    "- rating,\n",
    "- number of reviews,\n",
    "- link to visit details page,\n",
    "- address,\n",
    "- phone number (if existst),\n",
    "- website url (if exists),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "---\n",
    "### Web Scraping\n",
    "- [Web Scraping with Python](https://www.scrapethissite.com/pages/)\n",
    "\n",
    "### Beautiful Soup\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Beautiful Soup Tutorial](https://www.dataquest.io/blog/web-scraping-tutorial-python/)\n",
    "- [Beautiful Soup Tutorial Youtube](https://www.youtube.com/watch?v=ng2o98k983k)\n",
    "\n",
    "### Selenium\n",
    "- [Selenium Documentation](https://selenium-python.readthedocs.io/)\n",
    "- [Complex Selenium Tutorial in Java](https://www.guru99.com/selenium-tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
